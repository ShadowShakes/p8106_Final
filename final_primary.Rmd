---
title: "P8106 Final Project Primary Analysis"
author: "Shaohan Chen sc5154  XinRen xr2160"
date: "2023-04-28"
output: pdf_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE, message = FALSE, warning = FALSE)

library(corrplot)
library(leaps)
library(MASS)
library(pROC)
library(caret)
library(glmnet)
library(earth)
library(AppliedPredictiveModeling)
library(rpart.plot)
library(tidyverse)
```

In this part, we give the primary analysis of the recovery time prediction, where recovery time is taken as a continuous numeric variable.

# Data manipulate and tidy
```{r}
# load original dataset
load("recovery.rdata")

# generate random datasets and merge
set.seed(5154) 
df1 = dat[sample(1:10000, 2000),]
set.seed(2160)
df2 = dat[sample(1:10000, 2000),]
merge_df = rbind(df1, df2) %>% unique()

# tidy the dataset
covid_df = merge_df %>%
  janitor::clean_names() %>%
  na.omit() %>%
  mutate(
    gender = factor(gender, levels = c(0, 1)),
    race = factor(race, levels = c(1, 2, 3, 4)),
    smoking = factor(smoking, levels = c(0, 1, 2)),
    hypertension = factor(hypertension, levels = c(0, 1)),
    diabetes = factor(diabetes, levels = c(0, 1)),
    vaccine = factor(vaccine, levels = c(0, 1)),
    severity = factor(severity, levels = c(0, 1)),
    study = factor(study, levels = c("A", "B", "C")),
    ) %>%
  select(-id)

# check the distribution of response variable density
ggplot(dat, aes(x = recovery_time)) + 
  geom_density(color = "black", fill = "gray") + 
  labs(title =  "distribution of recovery time", x = "recovery time", y = "density") +
  theme(plot.title = element_text(hjust = 0.5, vjust = 0.5))

ggplot(dat, aes(x = log(recovery_time))) + 
  geom_density(color = "black", fill = "gray") + 
  labs(title =  "distribution of recovery time", x = "log recovery time", y = "density") +
  theme(plot.title = element_text(hjust = 0.5, vjust = 0.5))

# the distribution of response variable is very right-skewed, thus we adopt log transformation
covid_df = covid_df %>%
  mutate(recovery_time = log(recovery_time))

# data partition for training and testing
indexTrain = createDataPartition(y = covid_df$recovery_time, p = 0.8, list = FALSE)
train_df = covid_df[indexTrain, ]
test_df = covid_df[-indexTrain, ]

covid_df2 = model.matrix(recovery_time ~., covid_df)[, -1]
x = covid_df2[indexTrain, ]
y = covid_df$recovery_time[indexTrain]

# cv setting
ctrl = trainControl(method = "repeatedcv", number = 10, repeats = 5)
```


# Data Visualization and exploratory analysis
```{r}
# length of dataset
nrow(covid_df)
ncol(covid_df)

# data summary
summary(covid_df)

# feature plot
theme = trellis.par.get()
theme$plot.symbol$col = rgb(.2, .4, .2, .5)
theme$plot.symbol$pch = 16
theme$plot.line$col = rgb(.8, .1, .1, 1)
theme$plot.line$lwd = 2
theme$strip.background$col = rgb(.0, .2, .6, .2)
trellis.par.set(theme)

# feature plot of continuous variable
featurePlot(x = covid_df[ ,1:14] 
            %>% select(age, height, weight, bmi, sbp, ldl),
            y = covid_df[ ,15],
            plot = "scatter",
            span = .5,
            labels = c("Predictors","Y"),
            type = c("p", "smooth"),
            layout = c(3, 1))

# feature plot of categorical variables
cate_df = covid_df %>%
  select(-age, -height, -weight, -bmi, -sbp, -ldl) %>%
  pivot_longer(
   gender:study,
   names_to = "type",
   values_to = "category"
  )

cate_part1 = cate_df %>%
  filter(type %in% c("diabetes", "gender", "hypertension", "race"))

cate_part2 = cate_df %>%
  filter(type %in% c("severity", "smoking", "study", "vaccine"))
  
ggplot(cate_part1, aes(category, recovery_time, fill = type)) + 
  geom_boxplot() + 
  theme(axis.text.x = element_text(angle = 60, vjust = 1, hjust = 1)) + 
  facet_grid(. ~type, scales = "free_x")

ggplot(cate_part2, aes(category, recovery_time, fill = type)) + 
  geom_boxplot() + 
  theme(axis.text.x = element_text(angle = 60, vjust = 1, hjust = 1)) + 
  facet_grid(. ~type, scales = "free_x")

# correlation visualization
cor_df = covid_df %>% 
  select(age, height, weight, bmi, sbp, ldl)

corrplot(cor(cor_df), 
         method = "circle", 
         type = "lower",
         diag = FALSE,
         tl.cex = 0.5)
```

# Model Fitting

## KNN Model
```{r}
set.seed(5154)

kGrid = expand.grid(k = seq(from = 1, to = 40, by = 1))

# knn model
fit_knn = train(x, y,
                method = "knn",
                trControl = ctrl,
                preProcess = c("center", "scale"),
                tuneGrid = kGrid
                )

# model visual
ggplot(fit_knn)

# best k
fit_knn$finalModel$k

# test error
pred_knn = predict(fit_knn, newdata = x)
test_error_knn = mean((y - pred_knn)^2)
test_error_knn
```

## Linear Model Series

In this section, we adopt linear models including linear regression, Ridge regression and Lasso regression.

### Linear Regression
```{r}
# multivariate linear regression
set.seed(5154)

# linear regression
fit_lm = train(x, y,
               method = "lm",
               trControl = ctrl,
               preProcess = c("center", "scale"))

# model summary
summary(fit_lm)

# importance
plot(varImp(fit_lm, scale = TRUE))

# test error
pred_lm = predict(fit_lm, newdata = dat2[-indexTrain, ])
test_error_lm = mean((pred_lm - test_df$recovery_time)^2)
test_error_lm
```


### Ridge Regression
```{r}
# Ridge regression
set.seed(5154)
fit_ridge = train(x, y,
                  method = "glmnet",
                  tuneGrid = expand.grid(alpha = 0,
                                         lambda = exp(seq(8, -6, length = 100))),
                  trControl = ctrl,
                  preProcess = c("center", "scale"))

plot(fit_ridge, xTrans = log, xlab = "log(lambda)")

fit_ridge$bestTune
coef(fit_ridge$finalModel, s = fit_ridge$bestTune$lambda)

# importance
plot(varImp(fit_ridge, scale = TRUE))

# test error1
pred_ridge = predict(fit_ridge, newdata = dat2[-indexTrain, ])
error_ridge = mean((pred_ridge - dat$recovery_time[-indexTrain])^2)
error_ridge
```


### Lasso Regression
```{r}
# Lasso regression
set.seed(5154)
fit_lasso = train(x, y,
                  method = "glmnet",
                  tuneGrid = expand.grid(alpha = 1,
                                         lambda = exp(seq(4, -4, length = 50))),
                  trControl = ctrl,
                  preProcess = c("center", "scale"))

# model 
plot(fit_lasso, xTrans = log, xlab = "log(lambda)")

# best tune and non-zero coefficients
fit_lasso$bestTune
coef(fit_lasso$finalModel, s = fit_lasso$bestTune$lambda)

# variable importance
plot(varImp(fit_lasso, scale = TRUE))

# test error
pred_lasso = predict(fit_lasso, newdata = covid_df2[-indexTrain, ])
test_error_lasso = mean((pred_lasso - dat$recovery_time[-indexTrain])^2)
test_error_lasso
```


### Elastic Net
This is not included in final report.
```{r}
# Elastic net model
set.seed(5154)
fit_enet = train(x, y,
                  method = "glmnet",
                  tuneGrid = expand.grid(alpha = seq(0, 1, length = 21),
                                         lambda = exp(seq(2, -8, length = 50))),
                  trControl = ctrl,
                 preProcess = c("center", "scale"))

fit_enet$bestTune

myCol = rainbow(25)
myPar = list(superpose.symbol = list(col = myCol),
             superpose.line = list(col = myCol))
plot(fit_enet, par.settings = myPar)

# importance
plot(varImp(fit_enet, scale = TRUE))

coef(fit_enet$finalModel, s = fit_enet$bestTune$lambda)

# test error
pred_enet = predict(fit_enet, newdata = dat2[-indexTrain, ])
error_enet = mean((pred_enet - dat$recovery_time[-indexTrain])^2)
error_enet
```


## PCR Model
```{r}
set.seed(5154)
fit_pcr = train(x, y,
                method = "pcr",
                trControl = ctrl,
                tuneGrid = data.frame(ncomp = 1:19),
                preProcess = c("center", "scale"))

ggplot(fit_pcr, highlight = TRUE) + theme_bw()

coef(fit_pcr$finalModel)

# importance
plot(varImp(fit_pcr, scale = TRUE))

x2 = model.matrix(recovery_time ~., dat)[-indexTrain, -1]
y2 = dat$recovery_time[-indexTrain]

# test error
pred_pcr = predict(fit_pcr, newdata = x2)
error_pcr = mean((y2 - pred_pcr)^2)
error_pcr
```


## PLS Model
This is not included in final report.
```{r}
# PLS model
set.seed(5154)
fit_pls = train(x, y,
                method = "pls",
                tuneGrid = data.frame(ncomp = 1 : 19),
                trControl = ctrl,
                preProcess = c("center", "scale"))

ggplot(fit_pls, highlight = TRUE)

# importance
plot(varImp(fit_pls, scale = TRUE))

# test error
pred_pls = predict(fit_pls, newdata = x2)
error_pls = mean((y2 - pred_pls)^2)
error_pls
```


## GAM Model
```{r}
# GAM model
set.seed(5154)
fit_gam = train(x, y,
                method = "gam",
                trControl = ctrl,
                tuneGrid = data.frame(method = "GCV.Cp", select = c(TRUE,FALSE)),
                preProcess = c("center", "scale"))

fit_gam$bestTune
fit_gam$finalModel
fit_gam$finalModel$coefficients

# test error
pred_gam = predict(fit_gam, newdata = dat2[-indexTrain, ])
error_gam = mean((pred_gam - dat$recovery_time[-indexTrain])^2)
error_gam
```


## MARS Model
```{r}
# MARS model
mars_grid = expand.grid(degree = 1:3,
                        nprune = 2:15)
set.seed(5154)
fit_mars = train(x, y,
                 method = "earth",
                 tuneGrid = mars_grid,
                 trControl = ctrl,
                 preProcess = c("center", "scale"))

ggplot(fit_mars)
fit_mars$bestTune
fit_mars$finalModel
coef(fit_mars$finalModel)

# importance
plot(varImp(fit_mars, scale = TRUE))

# test error
pred_mars = predict(fit_mars, newdata = dat2[-indexTrain, ])
error_mars = mean((pred_mars - dat$recovery_time[-indexTrain])^2)
error_mars
```

## Tree Model

### CART Approach
```{r}
set.seed(5154)
fit_rpart = train(x, y,
                 method = "rpart",
                 tuneGrid = data.frame(cp = exp(seq(-6, -2, length = 50))),
                 trControl = ctrl,
                 preProcess = c("center", "scale"))

ggplot(fit_rpart, highlight = TRUE)
fit_rpart$bestTune$cp
rpart.plot(fit_rpart$finalModel)

# importance
plot(varImp(fit_rpart, scale = TRUE))

# test error
pred_rpart = predict(fit_rpart, newdata = dat2[-indexTrain, ])
error_rpart = mean((pred_rpart - dat$recovery_time[-indexTrain])^2)
error_rpart
```

### Bagging
```{r}
set.seed(5154)

rf_grid = expand.grid(mtry = 1:19,
                      splitrule = "variance",
                      min.node.size = 1:6)

fit_ranfo = train(recovery_time ~ . ,
               train_df,
               method = "ranger",
               tuneGrid = rf_grid,
               trControl = ctrl,
               preProcess = c("center", "scale"))

ggplot(fit_ranfo, highlight = TRUE)
```

### Boosting
```{r}
set.seed(5154)

gbm_grid = expand.grid(n.trees = c(500,1000,2000,3000,4000,5000),
                       interaction.depth = 1:3,
                       shrinkage = c(0.005,0.01),
                       n.minobsinnode = c(1))

fit_boost = train(recovery_time ~ . ,
               train_df,
               method = "gbm",
               tuneGrid = gbm_grid,
               trControl = ctrl,
               verbose = FALSE,
               preProcess = c("center", "scale"))

ggplot(fit_boost, highlight = TRUE)
```

### Supoort Vector Machine
```{r}
set.seed(5154)
```


# Model Selection
```{r}
# model comparison
set.seed(5154)
resamp = resamples(list(cart = fit_rpart,
                        mars = fit_mars, 
                        gam = fit_gam, 
                        pcr = fit_pcr,
                        lasso = fit_lasso, 
                        ridge = fit_ridge, 
                        lm = fit_lm))

# compare training
summary(resamp)
parallelplot(resamp, metric = "RMSE")
bwplot(resamp, metric = "RMSE")
bwplot(resamp, metric = "Rsquared")
```

We select model based on cross-validation error and interpretability.

# Model test performance.
We compare the test performance on test dataset. Reminder: this is only a comparison, we haven't used test data for model selection, unless you named the test data as "validation data".
```{r}
error_test = 
  tibble(
    cart = error_rpart,
    mars = error_mars, 
    gam = error_gam, 
    pcr = error_pcr,
    lasso = error_lasso, 
    ridge = error_ridge, 
    lm = error_lm,
  ) %>%
  pivot_longer(
    cart:lm,
    names_to = 'model',
    values_to = 'value'
  ) 

# ggplot(error_test, aes(x = model, y = value, fill = model)) +
#   geom_bar(stat = 'identity') + 
#   labs(
#     title = "Prediction error on testing set",
#     y = "RMSE"
#   )
```

\ \par

A summary of the above models are listed below:\par
\begin{tabular}{c|c|c}
\hline  
Model & Number of Parameters & Test Error  \\
\hline  
lm & 19  &  `r error_lm`\\
Lasso & 12 & `r error_lasso`\\
Ridge & 19 & `r error_ridge`\\
PCR & 18(comps) & `r error_pcr`\\
GAM & 19 & `r error_gam`\\
MARS & 9(terms) & `r error_mars`\\
CART & 5(internal nodes) & `r error_rpart`\\
\hline
\end{tabular}

```{r}
save.image(file = "midtermdata.RData")
```

